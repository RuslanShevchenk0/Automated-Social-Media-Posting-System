"""
–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Ç–µ–∫—Å—Ç—ñ–≤ –¥–ª—è –ø–æ—Å—Ç—ñ–≤ —É Facebook
+ AI-–∞–Ω–∞–ª—ñ–∑ —É—Å–ø—ñ—à–Ω–∏—Ö –ø–æ—Å—Ç—ñ–≤ –¥–ª—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ–π
"""

import asyncio
import logging
import json
from typing import List, Dict
from g4f.client import Client
from g4f.Provider import DeepInfra

logger = logging.getLogger(__name__)

async def generate_post_text(prompt: str, min_length: int = 50, max_length: int = 500,
                            max_retries: int = 3, use_recommendations: bool = True, lang: str = None) -> str:
    """
    –ì–µ–Ω–µ—Ä—É—î —Ç–µ–∫—Å—Ç –ø–æ—Å—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤—ñ –ø—Ä–æ–º–ø—Ç—É –∑ —É—Ä–∞—Ö—É–≤–∞–Ω–Ω—è–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ–π

    Args:
        prompt: —Ç–µ–º–∞ –∞–±–æ –ø—Ä–æ–º–ø—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó
        min_length: –º—ñ–Ω—ñ–º–∞–ª—å–Ω–∞ –¥–æ–≤–∂–∏–Ω–∞ —Ç–µ–∫—Å—Ç—É
        max_length: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –¥–æ–≤–∂–∏–Ω–∞ —Ç–µ–∫—Å—Ç—É
        max_retries: –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Å–ø—Ä–æ–±
        use_recommendations: –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó AI
        lang: –º–æ–≤–∞ ('en' –∞–±–æ 'uk'), —è–∫—â–æ None - –≤–∏–∑–Ω–∞—á–∞—î—Ç—å—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ

    Returns:
        str: –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–∏–π —Ç–µ–∫—Å—Ç
    """

    # –í–∏–∑–Ω–∞—á–∞—î–º–æ –º–æ–≤—É
    if lang:
        is_english = (lang == 'en')
    else:
        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–µ –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è –∑ –ø—Ä–æ–º–ø—Ç—É
        is_english = any(word in prompt.lower() for word in ['write', 'create', 'generate', 'make', 'style', 'tone', 'topics']) if prompt else False

    # –Ø–∫—â–æ –ø—Ä–æ–º–ø—Ç –ø–æ—Ä–æ–∂–Ω—ñ–π, –≥–µ–Ω–µ—Ä—É—î–º–æ –≤–∏–ø–∞–¥–∫–æ–≤—É —Ç–µ–º—É
    if not prompt or prompt.strip() == "":
        prompt = "Write a motivational post for social media" if is_english else "–ù–∞–ø–∏—à–∏ –º–æ—Ç–∏–≤–∞—Ü—ñ–π–Ω–∏–π –ø–æ—Å—Ç –¥–ª—è —Å–æ—Ü—ñ–∞–ª—å–Ω–∏—Ö –º–µ—Ä–µ–∂"

    # –ë–∞–∑–æ–≤–∏–π system prompt –∑–∞–ª–µ–∂–Ω–æ –≤—ñ–¥ –º–æ–≤–∏
    if is_english:
        system_content = f"You are a text generator for Facebook posts in English. Write meaningful posts with length from {min_length} to {max_length} characters."
    else:
        system_content = f"–¢–∏ - –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Ç–µ–∫—Å—Ç—ñ–≤ –¥–ª—è –ø–æ—Å—Ç—ñ–≤ —É Facebook —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é –º–æ–≤–æ—é. –ü–∏—à–∏ –∑–º—ñ—Å—Ç–æ–≤–Ω—ñ –ø–æ—Å—Ç–∏ –¥–æ–≤–∂–∏–Ω–æ—é –≤—ñ–¥ {min_length} –¥–æ {max_length} —Å–∏–º–≤–æ–ª—ñ–≤."

    # –Ø–∫—â–æ –≤–≤—ñ–º–∫–Ω–µ–Ω–æ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó, –∑–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ —Ç–∞ –¥–æ–¥–∞—î–º–æ —ó—Ö
    if use_recommendations:
        try:
            from database import db
            recommendation = db.get_latest_recommendation()

            if recommendation and recommendation.get('recommendations'):
                rec = recommendation['recommendations']

                # –î–æ–¥–∞—î–º–æ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –∑ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ–π –¥–æ system prompt
                if is_english:
                    system_content += "\n\nüìä Recommendations based on analysis of most successful posts:"
                else:
                    system_content += "\n\nüìä –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó –Ω–∞ –æ—Å–Ω–æ–≤—ñ –∞–Ω–∞–ª—ñ–∑—É –Ω–∞–π—É—Å–ø—ñ—à–Ω—ñ—à–∏—Ö –ø–æ—Å—Ç—ñ–≤:"

                # –°—Ç–∏–ª—å –∫–æ–Ω—Ç–µ–Ω—Ç—É
                if rec.get('ai_insights') and rec['ai_insights'].get('content_style'):
                    label = "Style" if is_english else "–°—Ç–∏–ª—å"
                    system_content += f"\n‚Ä¢ {label}: {rec['ai_insights']['content_style']}"

                # –¢–æ–Ω
                if rec.get('ai_insights') and rec['ai_insights'].get('tone'):
                    label = "Tone" if is_english else "–¢–æ–Ω"
                    system_content += f"\n‚Ä¢ {label}: {rec['ai_insights']['tone']}"

                # –ï—Ñ–µ–∫—Ç–∏–≤–Ω—ñ —Ç–µ–º–∏
                if rec.get('ai_insights') and rec['ai_insights'].get('effective_topics'):
                    topics = ', '.join(rec['ai_insights']['effective_topics'][:3])
                    label = "Effective topics" if is_english else "–ï—Ñ–µ–∫—Ç–∏–≤–Ω—ñ —Ç–µ–º–∏"
                    system_content += f"\n‚Ä¢ {label}: {topics}"

                # –ö–ª—é—á–æ–≤—ñ —Ñ—Ä–∞–∑–∏
                if rec.get('ai_insights') and rec['ai_insights'].get('key_phrases'):
                    phrases = ', '.join(rec['ai_insights']['key_phrases'][:5])
                    label = "Key phrases" if is_english else "–ö–ª—é—á–æ–≤—ñ —Ñ—Ä–∞–∑–∏"
                    system_content += f"\n‚Ä¢ {label}: {phrases}"

                # –°—Ç—Ä—É–∫—Ç—É—Ä–∞
                if rec.get('ai_insights') and rec['ai_insights'].get('structure_tips'):
                    label = "Structure" if is_english else "–°—Ç—Ä—É–∫—Ç—É—Ä–∞"
                    system_content += f"\n‚Ä¢ {label}: {rec['ai_insights']['structure_tips']}"

                # –ï–º–æ–¥–∂—ñ
                if rec.get('ai_insights') and rec['ai_insights'].get('emoji_usage'):
                    label = "Emoji" if is_english else "–ï–º–æ–¥–∂—ñ"
                    system_content += f"\n‚Ä¢ {label}: {rec['ai_insights']['emoji_usage']}"

                # –ó–∞–∫–ª–∏–∫–∏ –¥–æ –¥—ñ—ó
                if rec.get('ai_insights') and rec['ai_insights'].get('call_to_action'):
                    label = "Call to action" if is_english else "–ó–∞–∫–ª–∏–∫–∏ –¥–æ –¥—ñ—ó"
                    system_content += f"\n‚Ä¢ {label}: {rec['ai_insights']['call_to_action']}"

                # –î–æ–≤–∂–∏–Ω–∞ —Ç–µ–∫—Å—Ç—É –∑ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ–π
                if rec.get('text_length'):
                    rec_min = rec['text_length']['min']
                    rec_max = rec['text_length']['max']
                    label = "Optimal length" if is_english else "–û–ø—Ç–∏–º–∞–ª—å–Ω–∞ –¥–æ–≤–∂–∏–Ω–∞"
                    chars = "characters" if is_english else "—Å–∏–º–≤–æ–ª—ñ–≤"
                    system_content += f"\n‚Ä¢ {label}: {rec_min}-{rec_max} {chars}"

                logger.info("‚úì –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó –¥–æ–¥–∞–Ω–æ –¥–æ –ø—Ä–æ–º–ø—Ç—É –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó")

        except Exception as e:
            logger.warning(f"–ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó: {str(e)}")
            # –ü—Ä–æ–¥–æ–≤–∂—É—î–º–æ –±–µ–∑ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ–π
    
    for attempt in range(max_retries):
        try:
            logger.info(f"–°–ø—Ä–æ–±–∞ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó {attempt + 1}/{max_retries}")
            
            client = Client(provider=DeepInfra)
            response = await asyncio.to_thread(
                client.chat.completions.create,
                model="Qwen/Qwen3-Coder-30B-A3B-Instruct",
                messages=[
                    {"role": "system", "content": system_content},
                    {"role": "user", "content": prompt}
                ],
                web_search=False
            )
            
            content = response.choices[0].message.content.strip()
            
            if not content:
                logger.warning("–û—Ç—Ä–∏–º–∞–Ω–æ –ø–æ—Ä–æ–∂–Ω—é –≤—ñ–¥–ø–æ–≤—ñ–¥—å")
                if attempt < max_retries - 1:
                    await asyncio.sleep(2)
                    continue
                return "[–ü–æ–º–∏–ª–∫–∞: –ø–æ—Ä–æ–∂–Ω—è –≤—ñ–¥–ø–æ–≤—ñ–¥—å]"
            
            logger.info(f"‚úì –ó–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–æ —Ç–µ–∫—Å—Ç –¥–æ–≤–∂–∏–Ω–æ—é {len(content)} —Å–∏–º–≤–æ–ª—ñ–≤")
            return content
            
        except Exception as e:
            logger.error(f"–ü–æ–º–∏–ª–∫–∞ –≤ —Å–ø—Ä–æ–±—ñ {attempt + 1}: {str(e)}")
            if attempt < max_retries - 1:
                await asyncio.sleep(2)
            else:
                return f"[–ü–æ–º–∏–ª–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó: {str(e)}]"
    
    return "[–ù–µ –≤–¥–∞–ª–æ—Å—è –∑–≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ —Ç–µ–∫—Å—Ç]"


def analyze_successful_posts_sync(top_posts: List[Dict], lang: str = 'uk') -> Dict:
    """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞ –≤–µ—Ä—Å—ñ—è –∞–Ω–∞–ª—ñ–∑—É –ø–æ—Å—Ç—ñ–≤"""
    try:
        from g4f.client import Client
        from g4f.Provider import DeepInfra

        if not top_posts or len(top_posts) < 3:
            error_msg = 'Need at least 3 posts' if lang == 'en' else '–ü–æ—Ç—Ä—ñ–±–Ω–æ –º—ñ–Ω—ñ–º—É–º 3 –ø–æ—Å—Ç–∏'
            return {'success': False, 'error': error_msg}

        logger.info(f"AI-–∞–Ω–∞–ª—ñ–∑ {len(top_posts)} –ø–æ—Å—Ç—ñ–≤...")

        # Detect language from post content
        if lang == 'auto':
            sample_text = ' '.join([p.get('content', '')[:100] for p in top_posts[:3]])
            en_indicators = ['the', 'and', 'for', 'with', 'our', 'your', 'this', 'that', 'are', 'was', 'were', 'have', 'has', 'been']
            lang = 'en' if any(f' {word} ' in f' {sample_text.lower()} ' for word in en_indicators) else 'uk'

        posts_data = []
        for i, post in enumerate(top_posts[:10], 1):
            if lang == 'en':
                posts_data.append({
                    'number': i,
                    'text': post.get('content', '')[:200],
                    'length': post.get('text_length', 0),
                    'engagement_rate': round(post.get('avg_engagement_rate', 0), 4),
                    'likes': post.get('total_likes', 0),
                    'comments': post.get('total_comments', 0)
                })
            else:
                posts_data.append({
                    '–Ω–æ–º–µ—Ä': i,
                    '—Ç–µ–∫—Å—Ç': post.get('content', '')[:200],
                    '–¥–æ–≤–∂–∏–Ω–∞': post.get('text_length', 0),
                    'engagement_rate': round(post.get('avg_engagement_rate', 0), 4),
                    '–ª–∞–π–∫–∏': post.get('total_likes', 0),
                    '–∫–æ–º–µ–Ω—Ç–∞—Ä—ñ': post.get('total_comments', 0)
                })

        if lang == 'en':
            prompt = f"""Analyze {len(posts_data)} most successful posts:

{json.dumps(posts_data, ensure_ascii=False, indent=2)}

Respond ONLY with JSON:
{{
  "content_style": "style description",
  "effective_topics": ["topic1", "topic2"],
  "key_phrases": ["phrase1", "phrase2"],
  "tone": "tone description",
  "structure_tips": "tips",
  "emoji_usage": "how to use",
  "call_to_action": "recommendations"
}}"""
            system_msg = "You are a content analysis expert. Respond ONLY with JSON."
        else:
            prompt = f"""–ü—Ä–æ–∞–Ω–∞–ª—ñ–∑—É–π {len(posts_data)} –Ω–∞–π—É—Å–ø—ñ—à–Ω—ñ—à–∏—Ö –ø–æ—Å—Ç—ñ–≤:

{json.dumps(posts_data, ensure_ascii=False, indent=2)}

–í—ñ–¥–ø–æ–≤—ñ–¥–∞–π –¢–Ü–õ–¨–ö–ò JSON:
{{
  "content_style": "—Å—Ç–∏–ª—å",
  "effective_topics": ["—Ç–µ–º–∞1", "—Ç–µ–º–∞2"],
  "key_phrases": ["—Ñ—Ä–∞–∑–∞1", "—Ñ—Ä–∞–∑–∞2"],
  "tone": "—Ç–æ–Ω",
  "structure_tips": "–ø–æ—Ä–∞–¥–∏",
  "emoji_usage": "—è–∫",
  "call_to_action": "—á–∏ –ø–æ—Ç—Ä—ñ–±–Ω—ñ"
}}"""
            system_msg = "–¢–∏ –µ–∫—Å–ø–µ—Ä—Ç –∑ –∞–Ω–∞–ª—ñ–∑—É –∫–æ–Ω—Ç–µ–Ω—Ç—É. –í—ñ–¥–ø–æ–≤—ñ–¥–∞–π –¢–Ü–õ–¨–ö–ò JSON."

        client = Client(provider=DeepInfra)
        response = client.chat.completions.create(
            model="Qwen/Qwen3-Coder-30B-A3B-Instruct",
            messages=[
                {"role": "system", "content": system_msg},
                {"role": "user", "content": prompt}
            ],
            web_search=False
        )

        ai_response = response.choices[0].message.content.strip()
        ai_analysis = _parse_ai_response(ai_response)

        if ai_analysis:
            logger.info("‚úì AI-–∞–Ω–∞–ª—ñ–∑ –∑–∞–≤–µ—Ä—à–µ–Ω–æ")
            return {'success': True, 'analysis': ai_analysis, 'analyzed_posts_count': len(posts_data)}
        else:
            return {'success': False, 'error': '–ü–æ–º–∏–ª–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥—É'}

    except Exception as e:
        logger.error(f"–ü–æ–º–∏–ª–∫–∞ AI-–∞–Ω–∞–ª—ñ–∑—É: {str(e)}")
        return {'success': False, 'error': str(e)}


async def analyze_successful_posts(top_posts: List[Dict]) -> Dict:
    """
    –ê–Ω–∞–ª—ñ–∑—É—î —É—Å–ø—ñ—à–Ω—ñ –ø–æ—Å—Ç–∏ –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é AI –¥–ª—è –≤–∏—è–≤–ª–µ–Ω–Ω—è –ø–∞—Ç–µ—Ä–Ω—ñ–≤
    
    Args:
        top_posts: —Å–ø–∏—Å–æ–∫ —Ç–æ–ø-–ø–æ—Å—Ç—ñ–≤ –∑ –º–µ—Ç—Ä–∏–∫–∞–º–∏
    
    Returns:
        Dict: —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–≤–∞–Ω—ñ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó –≤—ñ–¥ AI
    """
    if not top_posts or len(top_posts) < 3:
        logger.warning("–ù–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ –ø–æ—Å—Ç—ñ–≤ –¥–ª—è AI-–∞–Ω–∞–ª—ñ–∑—É")
        return {
            'success': False,
            'error': '–ü–æ—Ç—Ä—ñ–±–Ω–æ –º—ñ–Ω—ñ–º—É–º 3 –ø–æ—Å—Ç–∏ –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É'
        }
    
    try:
        logger.info(f"–ü–æ—á–∞—Ç–æ–∫ AI-–∞–Ω–∞–ª—ñ–∑—É {len(top_posts)} —Ç–æ–ø-–ø–æ—Å—Ç—ñ–≤...")
        
        # –§–æ—Ä–º—É—î–º–æ –¥–∞–Ω—ñ –ø–æ—Å—Ç—ñ–≤ –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É
        posts_data = []
        for i, post in enumerate(top_posts[:10], 1):  # –ú–∞–∫—Å–∏–º—É–º 10 –ø–æ—Å—Ç—ñ–≤
            post_info = {
                '–Ω–æ–º–µ—Ä': i,
                '—Ç–µ–∫—Å—Ç': post.get('content', '')[:200],  # –ü–µ—Ä—à—ñ 200 —Å–∏–º–≤–æ–ª—ñ–≤
                '–¥–æ–≤–∂–∏–Ω–∞': post.get('text_length', 0),
                'engagement_rate': round(post.get('avg_engagement_rate', 0), 4),
                '–ª–∞–π–∫–∏': post.get('total_likes', 0),
                '–∫–æ–º–µ–Ω—Ç–∞—Ä—ñ': post.get('total_comments', 0),
                '—Ä–µ–ø–æ—Å—Ç–∏': post.get('total_shares', 0),
                '–ø–æ–∫–∞–∑–∏': post.get('total_impressions', 0),
                '—á–∞—Å_–ø—É–±–ª—ñ–∫–∞—Ü—ñ—ó': int(post.get('hour_of_day', 12)),
                '–¥–µ–Ω—å_—Ç–∏–∂–Ω—è': _get_day_name_uk(int(post.get('day_of_week', 0))),
                '—î_–ø–æ—Å–∏–ª–∞–Ω–Ω—è': bool(post.get('has_link', False)),
                '—î_–∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è': bool(post.get('has_images', False)),
                '–∫—ñ–ª—å–∫—ñ—Å—Ç—å_–∑–æ–±—Ä–∞–∂–µ–Ω—å': int(post.get('image_count', 0))
            }
            posts_data.append(post_info)
        
        # –§–æ—Ä–º—É—î–º–æ –ø—Ä–æ–º–ø—Ç –¥–ª—è AI
        prompt = f"""–ü—Ä–æ–∞–Ω–∞–ª—ñ–∑—É–π –Ω–∞—Å—Ç—É–ø–Ω—ñ {len(posts_data)} –Ω–∞–π—É—Å–ø—ñ—à–Ω—ñ—à–∏—Ö –ø–æ—Å—Ç—ñ–≤ —É Facebook –∑–∞ –æ—Å—Ç–∞–Ω–Ω—ñ–π –ø–µ—Ä—ñ–æ–¥:

{json.dumps(posts_data, ensure_ascii=False, indent=2)}

–¢–≤–æ—î –∑–∞–≤–¥–∞–Ω–Ω—è:
1. –í–∏–∑–Ω–∞—á —Å–ø—ñ–ª—å–Ω—ñ –ø–∞—Ç–µ—Ä–Ω–∏ —É—Å–ø—ñ—à–Ω–∏—Ö –ø–æ—Å—Ç—ñ–≤
2. –ü—Ä–æ–∞–Ω–∞–ª—ñ–∑—É–π —Å—Ç–∏–ª—å –Ω–∞–ø–∏—Å–∞–Ω–Ω—è, —Ç–µ–º–∏, —Å—Ç—Ä—É–∫—Ç—É—Ä—É
3. –í–∏–¥—ñ–ª–∏ –∫–ª—é—á–æ–≤—ñ —Ñ—Ä–∞–∑–∏ —Ç–∞ —Å–ª–æ–≤–∞, —â–æ —á–∞—Å—Ç–æ –∑—É—Å—Ç—Ä—ñ—á–∞—é—Ç—å—Å—è
4. –í–∏–∑–Ω–∞—á –Ω–∞–π–µ—Ñ–µ–∫—Ç–∏–≤–Ω—ñ—à—ñ –ø—ñ–¥—Ö–æ–¥–∏ –¥–æ –∫–æ–Ω—Ç–µ–Ω—Ç—É

–î–£–ñ–ï –í–ê–ñ–õ–ò–í–û: –í—ñ–¥–ø–æ–≤—ñ–¥–∞–π –¢–Ü–õ–¨–ö–ò –≤–∞–ª—ñ–¥–Ω–∏–º JSON —É —Ç–∞–∫–æ–º—É —Ñ–æ—Ä–º–∞—Ç—ñ (–±–µ–∑ –¥–æ–¥–∞—Ç–∫–æ–≤–æ–≥–æ —Ç–µ–∫—Å—Ç—É):
{{
  "content_style": "–∫–æ—Ä–æ—Ç–∫–∏–π –æ–ø–∏—Å —Å—Ç–∏–ª—é (1-2 —Ä–µ—á–µ–Ω–Ω—è)",
  "effective_topics": ["—Ç–µ–º–∞1", "—Ç–µ–º–∞2", "—Ç–µ–º–∞3"],
  "key_phrases": ["—Ñ—Ä–∞–∑–∞1", "—Ñ—Ä–∞–∑–∞2", "—Ñ—Ä–∞–∑–∞3"],
  "tone": "–æ–ø–∏—Å —Ç–æ–Ω—É (–¥—ñ–ª–æ–≤–∏–π/–Ω–µ—Ñ–æ—Ä–º–∞–ª—å–Ω–∏–π/–º–æ—Ç–∏–≤–∞—Ü—ñ–π–Ω–∏–π —Ç–æ—â–æ)",
  "structure_tips": "—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó –ø–æ —Å—Ç—Ä—É–∫—Ç—É—Ä—ñ –ø–æ—Å—Ç—ñ–≤ (2-3 —Ä–µ—á–µ–Ω–Ω—è)",
  "emoji_usage": "—è–∫ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ –µ–º–æ–¥–∂—ñ (–ø–æ–º—ñ—Ä–Ω–æ/–∞–∫—Ç–∏–≤–Ω–æ/–Ω–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏)",
  "call_to_action": "—á–∏ –ø–æ—Ç—Ä—ñ–±–Ω—ñ –∑–∞–∫–ª–∏–∫–∏ –¥–æ –¥—ñ—ó —Ç–∞ —è–∫—ñ"
}}

–í—ñ–¥–ø–æ–≤—ñ–¥–∞–π –õ–ò–®–ï JSON, –±–µ–∑ markdown, –±–µ–∑ –ø–æ—è—Å–Ω–µ–Ω—å!"""

        # –í–∏–∫–ª–∏–∫–∞—î–º–æ AI
        client = Client(provider=DeepInfra)
        response = await asyncio.to_thread(
            client.chat.completions.create,
            model="Qwen/Qwen3-Coder-30B-A3B-Instruct",
            messages=[
                {
                    "role": "system", 
                    "content": "–¢–∏ - –µ–∫—Å–ø–µ—Ä—Ç –∑ –∞–Ω–∞–ª—ñ–∑—É –∫–æ–Ω—Ç–µ–Ω—Ç—É —Å–æ—Ü—ñ–∞–ª—å–Ω–∏—Ö –º–µ—Ä–µ–∂. –ê–Ω–∞–ª—ñ–∑—É–π –ø–æ—Å—Ç–∏ —Ç–∞ –¥–∞–≤–∞–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–≤–∞–Ω—ñ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó —É —Ñ–æ—Ä–º–∞—Ç—ñ JSON. –í—ñ–¥–ø–æ–≤—ñ–¥–∞–π –¢–Ü–õ–¨–ö–ò –≤–∞–ª—ñ–¥–Ω–∏–º JSON –±–µ–∑ –¥–æ–¥–∞—Ç–∫–æ–≤–æ–≥–æ —Ç–µ–∫—Å—Ç—É."
                },
                {
                    "role": "user", 
                    "content": prompt
                }
            ],
            web_search=False
        )
        
        ai_response = response.choices[0].message.content.strip()
        logger.info(f"–û—Ç—Ä–∏–º–∞–Ω–æ –≤—ñ–¥–ø–æ–≤—ñ–¥—å –≤—ñ–¥ AI: {ai_response[:200]}...")
        
        # –ü–∞—Ä—Å–∏–º–æ JSON –≤—ñ–¥–ø–æ–≤—ñ–¥—å
        ai_analysis = _parse_ai_response(ai_response)
        
        if ai_analysis:
            logger.info("‚úì AI-–∞–Ω–∞–ª—ñ–∑ —É—Å–ø—ñ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–æ")
            return {
                'success': True,
                'analysis': ai_analysis,
                'analyzed_posts_count': len(posts_data)
            }
        else:
            logger.warning("–ù–µ –≤–¥–∞–ª–æ—Å—è —Ä–æ–∑–ø–∞—Ä—Å–∏—Ç–∏ –≤—ñ–¥–ø–æ–≤—ñ–¥—å AI")
            return {
                'success': False,
                'error': '–ù–µ –≤–¥–∞–ª–æ—Å—è —Ä–æ–∑–ø–∞—Ä—Å–∏—Ç–∏ –≤—ñ–¥–ø–æ–≤—ñ–¥—å AI'
            }
            
    except Exception as e:
        logger.error(f"–ü–æ–º–∏–ª–∫–∞ AI-–∞–Ω–∞–ª—ñ–∑—É: {str(e)}")
        return {
            'success': False,
            'error': str(e)
        }


def _parse_ai_response(response: str) -> Dict:
    """
    –ü–∞—Ä—Å–∏—Ç—å –≤—ñ–¥–ø–æ–≤—ñ–¥—å AI, –æ—á–∏—â–∞—é—á–∏ –≤—ñ–¥ markdown —Ç–∞ –∑–∞–π–≤–æ–≥–æ —Ç–µ–∫—Å—Ç—É
    
    Args:
        response: –≤—ñ–¥–ø–æ–≤—ñ–¥—å –≤—ñ–¥ AI
    
    Returns:
        Dict: —Ä–æ–∑–ø–∞—Ä—Å–µ–Ω–∏–π JSON –∞–±–æ None
    """
    try:
        # –í–∏–¥–∞–ª—è—î–º–æ markdown code blocks
        response = response.replace('```json', '').replace('```', '').strip()
        
        # –®—É–∫–∞—î–º–æ JSON –æ–±'—î–∫—Ç
        start = response.find('{')
        end = response.rfind('}') + 1
        
        if start != -1 and end > start:
            json_str = response[start:end]
            parsed = json.loads(json_str)
            return parsed
        
        # –Ø–∫—â–æ –Ω–µ –∑–Ω–∞–π—à–ª–∏, –ø—Ä–æ–±—É—î–º–æ –ø–∞—Ä—Å–∏—Ç–∏ —è–∫ —î
        return json.loads(response)
        
    except json.JSONDecodeError as e:
        logger.error(f"JSON parse error: {str(e)}")
        logger.error(f"Response was: {response[:500]}")
        return None
    except Exception as e:
        logger.error(f"Parse error: {str(e)}")
        return None


def _get_day_name_uk(day_index: int) -> str:
    """–ö–æ–Ω–≤–µ—Ä—Ç—É—î –Ω–æ–º–µ—Ä –¥–Ω—è –≤ —É–∫—Ä–∞—ó–Ω—Å—å–∫—É –Ω–∞–∑–≤—É"""
    days = ['–ø–æ–Ω–µ–¥—ñ–ª–æ–∫', '–≤—ñ–≤—Ç–æ—Ä–æ–∫', '—Å–µ—Ä–µ–¥–∞', '—á–µ—Ç–≤–µ—Ä', '–ø\'—è—Ç–Ω–∏—Ü—è', '—Å—É–±–æ—Ç–∞', '–Ω–µ–¥—ñ–ª—è']
    return days[day_index] if 0 <= day_index < 7 else '–Ω–µ–≤—ñ–¥–æ–º–æ'


async def generate_simple_post(topic: str = "") -> str:
    """
    –°–ø—Ä–æ—â–µ–Ω–∞ –≤–µ—Ä—Å—ñ—è –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó (—Ä–µ–∑–µ—Ä–≤–Ω–∏–π –≤–∞—Ä—ñ–∞–Ω—Ç)
    """
    import random
    
    templates = [
        f"‚ú® {topic}\n\n–¶–µ –≤–∞–∂–ª–∏–≤–∞ —Ç–µ–º–∞ –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –ø—ñ–¥–ø—Ä–∏—î–º—Ü—è. –î—ñ–∑–Ω–∞–π—Ç–µ—Å—è –±—ñ–ª—å—à–µ –ø—Ä–æ —Ç–µ, —è–∫ —Ü–µ –º–æ–∂–µ –≤–ø–ª–∏–Ω—É—Ç–∏ –Ω–∞ –≤–∞—à –±—ñ–∑–Ω–µ—Å.\n\n#–±—ñ–∑–Ω–µ—Å #–º–∞—Ä–∫–µ—Ç–∏–Ω–≥ #—É—Å–ø—ñ—Ö",

        f"üí° {topic}\n\n–¶–µ –º–æ–∂–µ —Å—Ç–∞—Ç–∏ –∫–ª—é—á–µ–º –¥–æ –≤–∞—à–æ–≥–æ —É—Å–ø—ñ—Ö—É. –ü–æ–¥—ñ–ª—ñ—Ç—å—Å—è —Å–≤–æ—ó–º–∏ –¥—É–º–∫–∞–º–∏ –≤ –∫–æ–º–µ–Ω—Ç–∞—Ä—è—Ö!\n\n#–ø–æ—Ä–∞–¥–∏ #—Ä–æ–∑–≤–∏—Ç–æ–∫ #–±—ñ–∑–Ω–µ—Å—Ç—Ä–µ–Ω–¥–∏",

        f"üöÄ {topic}\n\n–î–∞–≤–∞–π—Ç–µ –æ–±–≥–æ–≤–æ—Ä–∏–º–æ, —è–∫ —Ü–µ –∑–º—ñ–Ω—é—î —ñ–Ω–¥—É—Å—Ç—Ä—ñ—é —Ç–∞ —è–∫—ñ –º–æ–∂–ª–∏–≤–æ—Å—Ç—ñ –≤—ñ–¥–∫—Ä–∏–≤–∞—î.\n\n#—ñ–Ω–Ω–æ–≤–∞—Ü—ñ—ó #—Ç—Ä–µ–Ω–¥–∏ #–º–∞—Ä–∫–µ—Ç–∏–Ω–≥"
    ]
    
    return random.choice(templates)


# –¢–µ—Å—Ç–æ–≤–∞ —Ñ—É–Ω–∫—Ü—ñ—è
async def test_generation():
    """–®–≤–∏–¥–∫–∏–π —Ç–µ—Å—Ç –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó"""
    print("–¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ –ø–æ—Å—Ç—ñ–≤...\n")
    
    test_prompts = [
        "–ø–µ—Ä–µ–≤–∞–≥–∏ —Å–æ—Ü—ñ–∞–ª—å–Ω–∏—Ö –º–µ—Ä–µ–∂ –¥–ª—è –±—ñ–∑–Ω–µ—Å—É",
        "—è–∫ –ø—ñ–¥–≤–∏—â–∏—Ç–∏ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å",
        "–º–æ—Ç–∏–≤–∞—Ü—ñ–π–Ω–∞ –¥—É–º–∫–∞ –¥–Ω—è"
    ]
    
    for i, prompt in enumerate(test_prompts, 1):
        print(f"\n{'='*60}")
        print(f"–¢–µ—Å—Ç {i}: {prompt}")
        print('='*60)
        
        text = await generate_post_text(prompt, min_length=100, max_length=300)
        print(f"\n–†–µ–∑—É–ª—å—Ç–∞—Ç ({len(text)} —Å–∏–º–≤–æ–ª—ñ–≤):\n")
        print(text)
        print()


if __name__ == "__main__":
    asyncio.run(test_generation())